{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flexibility Algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm to Automate Flexibility Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "from functools import reduce\n",
    "import openpyxl\n",
    "import xlsxwriter\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import ngrams\n",
    "from nltk import ngrams, FreqDist\n",
    "from nltk.lm import NgramCounter\n",
    "import string\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "from nltk.cluster.kmeans import KMeansClusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put Data from Excel Sheet into Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual df's for each sheet\n",
    "\n",
    "# when on pc\n",
    "data_test_cup = pd.read_csv(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/test/semdis/autdata_test_cup_semdis.csv\")\n",
    "data_test_key = pd.read_csv(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/test/semdis/autdata_test_key_semdis.csv\")\n",
    "data_test_rope = pd.read_csv(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/test/semdis/autdata_test_rope_semdis.csv\")\n",
    "data_test_brick = pd.read_csv(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/test/semdis/autdata_test_brick_semdis.csv\")\n",
    "data_test_chair = pd.read_csv(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/test/semdis/autdata_test_chair_semdis.csv\")\n",
    "data_test_pencil = pd.read_csv(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/test/semdis/autdata_test_pencil_semdis.csv\")\n",
    "data_test_shoe = pd.read_csv(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/test/semdis/autdata_test_shoe_semdis.csv\")\n",
    "\n",
    "# when on mac\n",
    "# data_test_cup = pd.read_csv(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/test/semdis/autdata_test_cup_semdis.csv.xlsx\")\n",
    "# data_test_key = pd.read_csv(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/test/semdis/autdata_test_key_semdis.csv.xlsx\")\n",
    "# data_test_rope = pd.read_csv(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/test/semdis/autdata_test_rope_semdis.csv.xlsx\")\n",
    "# data_test_brick = pd.read_csv(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/test/semdis/autdata_test_brick_semdis.csv.xlsx\")\n",
    "# data_test_chair = pd.read_csv(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/test/semdis/autdata_test_chair_semdis.csv.xlsx\")\n",
    "# data_test_pencil = pd.read_csv(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/test/semdis/autdata_test_pencil_semdis.csv.xlsx\")\n",
    "# data_test_shoe = pd.read_csv(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/test/semdis/autdata_test_shoe_semdis.csv.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk corpus stop words\n",
    "stopwords_nltk = stopwords.words('english')\n",
    "# spacy stop words\n",
    "stopwords_spacy = STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_edited = list(stopwords_spacy)\n",
    "stopwords_edited.append(\"thing\")\n",
    "stopwords_edited.append(\"things\")\n",
    "stopwords_edited.append(\"use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to clean the responses\n",
    "def process_text(text, stopwords_list, remove_sw, join_list):\n",
    "    # tokenize text, lemmanize words, removing punctuation, remove stop words, lowercase all words\n",
    "\n",
    "    # hardcorded for special situations\n",
    "    text = re.sub(\"wedging\",\"wedge\", text)\n",
    "    text = re.sub(\"exersizing\",\"exercising\", text)\n",
    "    text = re.sub(\"thrown\",\"throw\", text)\n",
    "    \n",
    "    text = re.sub(\"/|-\",\" \", text)\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "    if remove_sw:\n",
    "        tokens = [word for word in tokens if word not in stopwords_list]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "#         stemmer = PorterStemmer()\n",
    "#         tokens = [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "    if join_list:\n",
    "        tokens = ' '.join(tokens)\n",
    " \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to get a list of participants\n",
    "def get_id_list(df):\n",
    "    id_list = df['id'].unique()\n",
    "    id_list = sorted(id_list)\n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to add a new column\n",
    "# new column are cleaned responses\n",
    "def get_cleaned_responses(df, stopwords_list, remove_sw, join_list):\n",
    "    # id_df = df[df.id == id]\n",
    "    df_processed = df.copy(deep=True)\n",
    "    responses = df['response'].tolist()\n",
    "\n",
    "    # make list of processed responses\n",
    "    for response in range(len(responses)):\n",
    "        responses[response] = process_text(responses[response], stopwords_list, remove_sw, join_list)\n",
    "\n",
    "    # add list as column in df\n",
    "    df_processed['response_processed'] = responses\n",
    "\n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Models for Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model\n",
    "word_model_twitter25 = api.load(\"glove-twitter-25\")\n",
    "\n",
    "# on pc\n",
    "# word_model_google = KeyedVectors.load_word2vec_format(\"C:/Users/jhec8/Documents/Northwestern_SROP/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "\n",
    "# on mac\n",
    "# word_model = KeyedVectors.load_word2vec_format(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of counts for each word in model\n",
    "twitter25_dict = {}\n",
    "for i in range(len(word_model_twitter25)):\n",
    "    twitter25_dict[word_model_twitter25.index_to_key[i]] = word_model_twitter25.key_to_index[word_model_twitter25.index_to_key[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the frequency of each word in dictionary\n",
    "total_words = 0\n",
    "for key in twitter25_dict:\n",
    "    total_words = total_words + twitter25_dict[key]\n",
    "    \n",
    "for key in twitter25_dict:\n",
    "    twitter25_dict[key] = twitter25_dict[key]/total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flexibility Algo 1\n",
    "### tf-idf scikit-learn + clustering\n",
    "### ___ as document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_vector(num_clusters, responses):\n",
    "    # initialize CountVectorizer object\n",
    "    tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "    # vectorize the phrases\n",
    "    tfidf = tfidf_vectorizer.fit_transform(responses)\n",
    "    \n",
    "    # elbow method to visualize and find out how many clusters to use\n",
    "#     visualizer = KElbowVisualizer(KMeans(), k=(1,12), timings=False)\n",
    "#     visualizer.fit(tfidf.toarray())       \n",
    "#     visualizer.show()\n",
    "\n",
    "    # nltk kmeans cosine distance implementation\n",
    "    number_of_clusters = num_clusters\n",
    "    kmeans = KMeansClusterer(number_of_clusters, distance=nltk.cluster.util.cosine_distance, repeats=25, avoid_empty_clusters=True)\n",
    "#     print(tfidf.toarray().reshape(450,283).tolist())\n",
    "    assigned_clusters = kmeans.cluster(tfidf.toarray(), assign_clusters=True)\n",
    "#     print(\"HERE\")\n",
    "#     scikit-learn euclidean distance implementation\n",
    "#     kmeans = KMeans(n_clusters = num_clusters).fit(tfidf)\n",
    "        \n",
    "    # cluster results scikit-learn\n",
    "    results = pd.DataFrame()\n",
    "    results['text'] = responses\n",
    "#     results['category'] = kmeans.labels_\n",
    "    results['category'] = assigned_clusters\n",
    "    \n",
    "    # create dictionary to organize the clusters with their respective phrases\n",
    "    results_dict = {k: g[\"text\"].tolist() for k,g in results.groupby(\"category\")}\n",
    "    \n",
    "    # df of the clusters and the \n",
    "    clusters_df = pd.DataFrame(list(results_dict.items()),columns = ['category','responses']) \n",
    "    \n",
    "    # uncomment to see clusters \n",
    "#     print(results_dict)\n",
    "#     display(clusters_df)\n",
    "    \n",
    "    return clusters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flexibility_score(flexibility_rating_df, num_clusters, responses):\n",
    "    clusters_df = get_tfidf_vector(num_clusters, responses)\n",
    "    \n",
    "    # create dictionary out of cluster df\n",
    "    # has clusters and their respective responses\n",
    "    clusters = dict(zip(clusters_df.category, clusters_df.responses))\n",
    "    \n",
    "    flex_df_cleaned = flexibility_rating_df[flexibility_rating_df.response_processed != '']\n",
    "    participants = get_id_list(flexibility_rating_df)\n",
    "    participants_responses_list = list(zip(flex_df_cleaned.id, flex_df_cleaned.response_processed))\n",
    "    \n",
    "    # get dictionary of each participants responses\n",
    "    participants_responses_dict = {k: [] for k in participants}\n",
    "    \n",
    "    for index in range(len(participants_responses_list)):\n",
    "        participants_responses_dict[participants_responses_list[index][0]].append(participants_responses_list[index][1])\n",
    "        \n",
    "    # get dictionary of responses and their respective dictionary\n",
    "    responses_cluster_rep = {}\n",
    "    \n",
    "    for key in clusters:\n",
    "        for phrase in clusters[key]:\n",
    "            responses_cluster_rep[phrase] = key\n",
    "            \n",
    "    # get dictionary of participants and clusters their responses existed in\n",
    "    participants_clusters_apperance = {k: [] for k in participants}\n",
    "    \n",
    "    for index in range(len(participants_responses_list)):\n",
    "        participants_clusters_apperance[participants_responses_list[index][0]].append(responses_cluster_rep[participants_responses_list[index][1]])\n",
    "        \n",
    "    # get dic of number of clusters a participants responses are in\n",
    "    \n",
    "    participants_clusters_seen = {k: [] for k in participants}\n",
    "    \n",
    "    for participant in participants_clusters_seen:\n",
    "        responses_set = set(participants_clusters_apperance[participant])\n",
    "        participants_clusters_seen[participant] = len(responses_set)\n",
    "    \n",
    "#     print(clusters)\n",
    "#     print()\n",
    "#     print(participants_responses_list)\n",
    "#     print()\n",
    "#     print(participants_responses_dict)\n",
    "#     print()\n",
    "#     print(responses_cluster_rep)\n",
    "#     print()\n",
    "#     print(participants_clusters_apperance)\n",
    "#     print()\n",
    "#     print(participants_clusters_seen)\n",
    "    \n",
    "    flexibility_df = pd.DataFrame(participants_clusters_seen.items(), columns=['id', 'flexibility'])\n",
    "    return flexibility_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flexibility_tfidf_scikit_learn_clustering(df, stopwords_list, num_clusters, remove_sw, join_list):\n",
    "    flexibility_rating_df = get_cleaned_responses(df, stopwords_list, remove_sw, join_list)\n",
    "    responses_split = flexibility_rating_df['response_processed'].tolist()\n",
    "    responses_split = [word for word in responses_split if word != '']\n",
    "    responses = []\n",
    "    \n",
    "    id_list = get_id_list(df)\n",
    "    \n",
    "    for participant in id_list:\n",
    "        temp_df = flexibility_rating_df.loc[flexibility_rating_df['id'] == participant]\n",
    "        temp_list = temp_df['response_processed'].tolist()\n",
    "        temp_list = ' '.join(temp_list)\n",
    "        responses.append(temp_list)\n",
    "                \n",
    "    flexibility_rating_df = get_flexibility_score(flexibility_rating_df, num_clusters, responses_split)\n",
    "        \n",
    "    return flexibility_rating_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>flexibility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1087</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1093</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1094</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1102</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1104</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1603</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1610</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1614</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1621</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1622</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  flexibility\n",
       "0   1087            4\n",
       "1   1093            4\n",
       "2   1094            5\n",
       "3   1102            2\n",
       "4   1104            2\n",
       "..   ...          ...\n",
       "85  1603            3\n",
       "86  1610            5\n",
       "87  1614            2\n",
       "88  1621            5\n",
       "89  1622            3\n",
       "\n",
       "[90 rows x 2 columns]"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_flexibility_tfidf_scikit_learn_clustering(data_official_brick, stopwords_edited, 15, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_official_brick['response'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Algo Results with Human Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when on pc\n",
    "data_official_cup = pd.read_excel(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/flexibility/official/autdata_flex_results.xlsx\", sheet_name='Cup')\n",
    "data_official_key = pd.read_excel(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/flexibility/official/autdata_flex_results.xlsx\", sheet_name='Key')\n",
    "data_official_rope = pd.read_excel(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/flexibility/official/autdata_flex_results.xlsx\", sheet_name='Rope')\n",
    "data_official_brick = pd.read_excel(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/flexibility/official/autdata_flex_results.xlsx\", sheet_name='Brick')\n",
    "data_official_chair = pd.read_excel(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/flexibility/official/autdata_flex_results.xlsx\", sheet_name='Chair')\n",
    "data_official_pencil = pd.read_excel(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/flexibility/official/autdata_flex_results.xlsx\", sheet_name='Pencil')\n",
    "data_official_shoe = pd.read_excel(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/flexibility/official/autdata_flex_results.xlsx\", sheet_name='Shoe')\n",
    "data_official_box = pd.read_excel(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/flexibility/official/autdata_flex_results.xlsx\", sheet_name='Box')\n",
    "\n",
    "# when on mac\n",
    "# data_official_cup = pd.read_excel(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/flexibility/official/autdata_flex_results.xlsx\", sheet_name='Cup')\n",
    "# data_official_key = pd.read_excel(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/flexibility/official/autdata_flex_results.xlsx\", sheet_name='Key')\n",
    "# data_official_rope = pd.read_excel(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/flexibility/official/autdata_flex_results.xlsx\", sheet_name='Rope')\n",
    "# data_official_brick = pd.read_excel(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/flexibility/official/autdata_flex_results.xlsx\", sheet_name='Brick')\n",
    "# data_official_chair = pd.read_excel(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/flexibility/official/autdata_flex_results.xlsx\", sheet_name='Chair')\n",
    "# data_official_pencil = pd.read_excel(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/flexibility/official/autdata_flex_results.xlsx\", sheet_name='Pencil')\n",
    "# data_official_shoe = pd.read_excel(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/flexibility/official/autdata_flex_results.xlsx\", sheet_name='Shoe')\n",
    "# data_official_box = pd.read_excel(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/flexibility/official/autdata_flex_results.xlsx\", sheet_name='Box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_list = ['box', 'brick', 'chair', 'cup', 'key', 'pencil', 'rope', 'shoe']\n",
    "data_list = [data_official_box, data_official_brick, data_official_chair, data_official_cup, data_official_key, data_official_pencil, data_official_rope, data_official_shoe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>stim</th>\n",
       "      <th>response</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1094</td>\n",
       "      <td>Box</td>\n",
       "      <td>to store things</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1094</td>\n",
       "      <td>Box</td>\n",
       "      <td>to build a fort</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1094</td>\n",
       "      <td>Box</td>\n",
       "      <td>as an umbrella</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1094</td>\n",
       "      <td>Box</td>\n",
       "      <td>to catch a bunny</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id stim          response  Category\n",
       "0  1094  Box   to store things       1.0\n",
       "1  1094  Box   to build a fort       4.0\n",
       "2  1094  Box    as an umbrella       1.0\n",
       "3  1094  Box  to catch a bunny       4.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = data_official_box[data_official_box.id == 1094]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test['Category'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_flexibility_corrs():\n",
    "    flexibility_df_list = []\n",
    "    for i in range(len(prompts_list)):\n",
    "        print(prompts_list[i])\n",
    "        id_list = get_id_list(data_list[i])\n",
    "        participants_clusters_seen = {k: 0 for k in id_list}\n",
    "        for participant in participants_clusters_seen:\n",
    "            id_df = data_list[i][data_list[i].id == participant]\n",
    "            cluster_apperance = len(id_df['Category'].unique())\n",
    "            participants_clusters_seen[participant] = cluster_apperance\n",
    "        flexibility_df_rater = pd.DataFrame(participants_clusters_seen.items(), columns=['id', 'rating'])\n",
    "        flexibility_df_method = get_flexibility_tfidf_scikit_learn_clustering(data_list[i], stopwords_edited, 15, True, True)\n",
    "        df_cd = pd.merge(flexibility_df_rater, flexibility_df_method, how='inner', on = 'id')\n",
    "        flexibility_df_list.append(df_cd)\n",
    "        \n",
    "    return flexibility_df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "box\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jhec8\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\nltk\\cluster\\util.py:131: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return 1 - (numpy.dot(u, v) / (sqrt(numpy.dot(u, u)) * sqrt(numpy.dot(v, v))))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-235-97196e5ee279>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mflex_results_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprint_flexibility_corrs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-232-0fc8ba42eefc>\u001b[0m in \u001b[0;36mprint_flexibility_corrs\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[0mparticipants_clusters_seen\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparticipant\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcluster_apperance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mflexibility_df_rater\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparticipants_clusters_seen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rating'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mflexibility_df_method\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_flexibility_tfidf_scikit_learn_clustering\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstopwords_edited\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mdf_cd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflexibility_df_rater\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflexibility_df_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'inner'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mflexibility_df_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_cd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-230-d6e72e114193>\u001b[0m in \u001b[0;36mget_flexibility_tfidf_scikit_learn_clustering\u001b[1;34m(df, stopwords_list, num_clusters, remove_sw, join_list)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mresponses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mflexibility_rating_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_flexibility_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflexibility_rating_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_clusters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponses_split\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mflexibility_rating_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-229-3f4d79847abc>\u001b[0m in \u001b[0;36mget_flexibility_score\u001b[1;34m(flexibility_rating_df, num_clusters, responses)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_flexibility_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflexibility_rating_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_clusters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mclusters_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_tfidf_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_clusters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m# create dictionary out of cluster df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# has clusters and their respective responses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-228-20813d44d64e>\u001b[0m in \u001b[0;36mget_tfidf_vector\u001b[1;34m(num_clusters, responses)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mnumber_of_clusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_clusters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mkmeans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKMeansClusterer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber_of_clusters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcosine_distance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepeats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavoid_empty_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0massigned_clusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0massign_clusters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;31m#     scikit-learn euclidean distance implementation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#     kmeans = KMeans(n_clusters = num_clusters).fit(tfidf)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jhec8\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\nltk\\cluster\\util.py\u001b[0m in \u001b[0;36mcluster\u001b[1;34m(self, vectors, assign_clusters, trace)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;31m# call abstract method to cluster the vectors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcluster_vectorspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;31m# assign the vectors to clusters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jhec8\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\nltk\\cluster\\kmeans.py\u001b[0m in \u001b[0;36mcluster_vectorspace\u001b[1;34m(self, vectors, trace)\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_means\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_means\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_means\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cluster_vectorspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m             \u001b[0mmeanss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_means\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jhec8\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\nltk\\cluster\\kmeans.py\u001b[0m in \u001b[0;36m_cluster_vectorspace\u001b[1;34m(self, vectors, trace)\u001b[0m\n\u001b[0;32m    120\u001b[0m                 \u001b[0mclusters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_means\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mvector\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvectors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m                     \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify_vectorspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m                     \u001b[0mclusters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jhec8\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\nltk\\cluster\\kmeans.py\u001b[0m in \u001b[0;36mclassify_vectorspace\u001b[1;34m(self, vector)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_means\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_means\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m             \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbest_distance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdist\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mbest_distance\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m                 \u001b[0mbest_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_distance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jhec8\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\nltk\\cluster\\util.py\u001b[0m in \u001b[0;36mcosine_distance\u001b[1;34m(u, v)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[0mequal\u001b[0m \u001b[0mto\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m|\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m|\u001b[0m\u001b[1;33m|\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m|\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \"\"\"\n\u001b[1;32m--> 131\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "flex_results_list = print_flexibility_corrs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Results into Excel Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_list = ['cup', 'key', 'rope', 'brick', 'chair', 'pencil', 'shoe']\n",
    "data_list = [data_test_cup, data_test_key, data_test_rope, data_test_brick, data_test_chair, data_test_pencil, data_test_shoe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write results df of each dataset for specific methods\n",
    "# def write_results_excel(method, stopwords, remove_sw, join_list):\n",
    "#     # change this when writing new sheet\n",
    "#     writer = pd.ExcelWriter('originality_cv_freq_cluster_results.xlsx', engine='xlsxwriter')\n",
    "\n",
    "#     for i in range(len(prompts_list)):\n",
    "#         df = method(data_list[i], stopwords, remove_sw, join_list)\n",
    "#         df.to_excel(writer, sheet_name = prompts_list[i], index = False)\n",
    "#     writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_results_excel(get_originality_count_vectorizer, stopwords_spacy, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algo Design Brainstorming:\n",
    "\n",
    "To Do List\n",
    "- [ ] brainstorm strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4fb9038e648b520597eaf423817a5ee4bf2adb29cdca822101c12482799b056"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
