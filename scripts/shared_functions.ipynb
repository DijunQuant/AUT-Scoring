{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40ccd6a0",
   "metadata": {},
   "source": [
    "# Shared Functions between the Different AUT Metric Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f54cda",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc5beee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33da9e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "global stopwords_edited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae843f66",
   "metadata": {},
   "source": [
    "## Preprocessing Data Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf41997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_edited = list(STOP_WORDS)\n",
    "stopwords_edited.append(\"thing\")\n",
    "stopwords_edited.append(\"use\")\n",
    "stopwords_edited.append(\"things\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "405d5f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to clean the responses\n",
    "def process_text(text, stopwords_list, join_list):\n",
    "    # tokenize text, lemmanize words, removing punctuation, remove stop words, lowercase all words\n",
    "\n",
    "    # hardcorded for special situations\n",
    "    text = re.sub(\"/|-\",\" \", text)\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "    tokens = [word for word in tokens if word not in stopwords_list]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    "    if join_list:\n",
    "        tokens = ' '.join(tokens)\n",
    " \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce3fad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to add a new column\n",
    "# new column are cleaned responses\n",
    "def get_cleaned_responses_df(df, stopwords_list, join_list):\n",
    "    # id_df = df[df.id == id]\n",
    "    df_processed = df.copy(deep=True)\n",
    "    responses = df['response'].tolist()\n",
    "\n",
    "    # make list of processed responses\n",
    "    for response in range(len(responses)):\n",
    "        responses[response] = process_text(responses[response], stopwords_list, join_list)\n",
    "\n",
    "    # add list as column in df\n",
    "    df_processed['response_processed'] = responses\n",
    "    \n",
    "    df_processed = df_processed[df_processed.astype(str)['response_processed'] != '[]']\n",
    "\n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d815bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be773e00",
   "metadata": {},
   "source": [
    "## Working with Dataframe Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cef9a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to get a list of participants\n",
    "def get_id_list(df):\n",
    "    id_list = df['id'].unique()\n",
    "    id_list = sorted(id_list)\n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbd484b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fdfcd94f",
   "metadata": {},
   "source": [
    "## Calculate Cosine Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16159156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to calculate cosine distance\n",
    "def get_cosine_distance(feature_vec_1, feature_vec_2):\n",
    "    return (1 - cosine_similarity(feature_vec_1.reshape(1, -1), feature_vec_2.reshape(1, -1))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f50618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fee7ad93",
   "metadata": {},
   "source": [
    "## Clustering Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2f2dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clusters the responses\n",
    "# get a df of the clusters and their respective phrases\n",
    "def get_counts_vector(num_clusters, responses, display_clusters):\n",
    "    # initialize CountVectorizer object\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    # vectorize the phrases\n",
    "    word_count = count_vectorizer.fit_transform(responses)\n",
    "    \n",
    "    # elbow method to visualize and find out how many clusters to use\n",
    "#     visualizer = KElbowVisualizer(KMeans(), k=(10,35), timings=False)\n",
    "#     visualizer.fit(word_count.toarray())       \n",
    "#     visualizer.show()\n",
    "\n",
    "    # nltk kmeans cosine distance implementation\n",
    "    number_of_clusters = num_clusters\n",
    "    kmeans = KMeansClusterer(number_of_clusters, distance=nltk.cluster.util.cosine_distance, repeats=25, avoid_empty_clusters=True)\n",
    "    assigned_clusters = kmeans.cluster(word_count.toarray(), assign_clusters=True)\n",
    "\n",
    "    # cluster results scikit-learn\n",
    "    results = pd.DataFrame()\n",
    "    results['text'] = responses\n",
    "#     results['category'] = kmeans.labels_\n",
    "    results['category'] = assigned_clusters\n",
    "    \n",
    "    # create dictionary to organize the clusters with their respective phrases\n",
    "    results_dict = {k: g[\"text\"].tolist() for k,g in results.groupby(\"category\")}\n",
    "    \n",
    "    # df of the clusters and the \n",
    "    clusters_df = pd.DataFrame(list(results_dict.items()),columns = ['category','responses']) \n",
    "    \n",
    "    if display_clusters:\n",
    "        display(clusters_df)\n",
    "    \n",
    "    return clusters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d11715fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_vector(num_clusters, responses, display_clusters):\n",
    "    # initialize CountVectorizer object\n",
    "    tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "    # vectorize the phrases\n",
    "    tfidf = tfidf_vectorizer.fit_transform(responses)\n",
    "    \n",
    "    # elbow method to visualize and find out how many clusters to use\n",
    "#     visualizer = KElbowVisualizer(KMeans(), k=(1,12), timings=False)\n",
    "#     visualizer.fit(tfidf.toarray())       \n",
    "#     visualizer.show()\n",
    "\n",
    "    # nltk kmeans cosine distance implementation\n",
    "    number_of_clusters = num_clusters\n",
    "    kmeans = KMeansClusterer(number_of_clusters, repeats=25, distance=nltk.cluster.util.cosine_distance, avoid_empty_clusters=True)\n",
    "    assigned_clusters = kmeans.cluster(tfidf.toarray(), assign_clusters=True)\n",
    "        \n",
    "    # cluster results scikit-learn\n",
    "    results = pd.DataFrame()\n",
    "    results['text'] = responses\n",
    "#     results['category'] = kmeans.labels_\n",
    "    results['category'] = assigned_clusters\n",
    "    \n",
    "    # create dictionary to organize the clusters with their respective phrases\n",
    "    results_dict = {k: g[\"text\"].tolist() for k,g in results.groupby(\"category\")}\n",
    "    \n",
    "    # df of the clusters and the \n",
    "    clusters_df = pd.DataFrame(list(results_dict.items()),columns = ['category','responses']) \n",
    "    \n",
    "    if display_clusters:\n",
    "        display(clusters_df)\n",
    "    \n",
    "    return clusters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d486b50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bdcfe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
