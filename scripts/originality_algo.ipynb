{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Originality Algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm to Automate Originality Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "from functools import reduce\n",
    "import openpyxl\n",
    "import xlsxwriter\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import ngrams\n",
    "from nltk import ngrams, FreqDist\n",
    "from nltk.lm import NgramCounter\n",
    "import string\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "from nltk.cluster.kmeans import KMeansClusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put Data from Excel Sheet into Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/autdata_test_cup_semdis.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-92ba42d26c72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# when on pc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdata_cup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/autdata_test_cup_semdis.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdata_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/autdata_test_key_semdis.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdata_rope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/autdata_test_rope_semdis.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jhec8\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jhec8\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jhec8\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jhec8\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jhec8\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jhec8\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\jhec8\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/autdata_test_cup_semdis.csv'"
     ]
    }
   ],
   "source": [
    "# individual df's for each sheet\n",
    "\n",
    "# when on pc\n",
    "data_cup = pd.read_csv(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/autdata_test_cup_semdis.csv\")\n",
    "data_key = pd.read_csv(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/autdata_test_key_semdis.csv\")\n",
    "data_rope = pd.read_csv(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/autdata_test_rope_semdis.csv\")\n",
    "data_brick = pd.read_csv(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/autdata_test_brick_semdis.csv\")\n",
    "data_chair = pd.read_csv(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/autdata_test_chair_semdis.csv\")\n",
    "data_pencil = pd.read_csv(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/autdata_test_pencil_semdis.csv\")\n",
    "data_shoe = pd.read_csv(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/autdata_test_shoe_semdis.csv\")\n",
    "\n",
    "# when on mac\n",
    "# data_cup = pd.read_csv(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/autdata_test_cup_semdis.csv.xlsx\")\n",
    "# data_key = pd.read_csv(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/autdata_test_key_semdis.csv.xlsx\")\n",
    "# data_rope = pd.read_csv(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/autdata_test_rope_semdis.csv.xlsx\")\n",
    "# data_brick = pd.read_csv(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/autdata_test_brick_semdis.csv.xlsx\")\n",
    "# data_chair = pd.read_csv(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/autdata_test_chair_semdis.csv.xlsx\")\n",
    "# data_pencil = pd.read_csv(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/autdata_test_pencil_semdis.csv.xlsx\")\n",
    "# data_shoe = pd.read_csv(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/autdata_test_shoe_semdis.csv.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk corpus stop words\n",
    "stopwords_nltk = stopwords.words('english')\n",
    "# spacy stop words\n",
    "stopwords_spacy = STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to clean the responses\n",
    "def process_text(text, stopwords_list, remove_sw, join_list):\n",
    "    # tokenize text, lemmanize words, removing punctuation, remove stop words, lowercase all words\n",
    "\n",
    "    # hardcorded for special situations\n",
    "    text = re.sub(\"wedging\",\"wedge\", text)\n",
    "    text = re.sub(\"exersizing\",\"exercising\", text)\n",
    "    text = re.sub(\"thrown\",\"throw\", text)\n",
    "    \n",
    "    text = re.sub(\"/|-\",\" \", text)\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    \n",
    "    if remove_sw:\n",
    "        tokens = [word for word in tokens if word not in stopwords_list]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "#         stemmer = PorterStemmer()\n",
    "#         tokens = [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "    if join_list:\n",
    "        tokens = ' '.join(tokens)\n",
    " \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to get a list of participants\n",
    "def get_id_list(df):\n",
    "    id_list = df['id'].unique()\n",
    "    id_list = sorted(id_list)\n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to add a new column\n",
    "# new column are cleaned responses\n",
    "def get_cleaned_responses(df, stopwords_list, remove_sw, join_list):\n",
    "    # id_df = df[df.id == id]\n",
    "    df_processed = df.copy(deep=True)\n",
    "    responses = df['response'].tolist()\n",
    "\n",
    "    # make list of processed responses\n",
    "    for response in range(len(responses)):\n",
    "        responses[response] = process_text(responses[response], stopwords_list, remove_sw, join_list)\n",
    "\n",
    "    # add list as column in df\n",
    "    df_processed['response_processed'] = responses\n",
    "\n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Models for Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model\n",
    "word_model_twitter25 = api.load(\"glove-twitter-25\")\n",
    "\n",
    "# on pc\n",
    "# word_model_google = KeyedVectors.load_word2vec_format(\"C:/Users/jhec8/Documents/Northwestern_SROP/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "\n",
    "# on mac\n",
    "# word_model = KeyedVectors.load_word2vec_format(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of counts for each word in model\n",
    "twitter25_dict = {}\n",
    "for i in range(len(word_model_twitter25)):\n",
    "    twitter25_dict[word_model_twitter25.index_to_key[i]] = word_model_twitter25.key_to_index[word_model_twitter25.index_to_key[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the frequency of each word in dictionary\n",
    "total_words = 0\n",
    "for key in twitter25_dict:\n",
    "    total_words = total_words + twitter25_dict[key]\n",
    "    \n",
    "for key in twitter25_dict:\n",
    "    twitter25_dict[key] = twitter25_dict[key]/total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Originality Algo 1\n",
    "### Term Frequency Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_dict(responses):\n",
    "    cv = CountVectorizer()   \n",
    "    cv_fit = cv.fit_transform(responses)    \n",
    "    word_list = cv.get_feature_names()\n",
    "\n",
    "    # [0] here to get a 1d-array for iteration by the zip function \n",
    "    count_list = np.asarray(cv_fit.sum(axis=0))[0]\n",
    "\n",
    "    tf_dict = dict(zip(word_list, count_list))\n",
    "    \n",
    "    total_words = 0\n",
    "    for key in tf_dict:\n",
    "        total_words = total_words + tf_dict[key]\n",
    "        \n",
    "    for key in tf_dict:\n",
    "        tf_dict[key] = np.log(1 + tf_dict[key]/total_words)\n",
    "        \n",
    "    tf_dict = dict(sorted(tf_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "                \n",
    "    return tf_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_sum(tf_dict, response):\n",
    "    tf_sum = 0\n",
    "    for term in range(len(response)):\n",
    "        tf_sum = tf_sum + tf_dict[response[term]]\n",
    "        \n",
    "    if tf_sum == 0:\n",
    "        tf_sum = np.nan\n",
    "        \n",
    "    return tf_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_sum_list(tf_dict, responses):\n",
    "    tf_sum_list = []\n",
    "    \n",
    "    for response in responses:\n",
    "        tf_sum_list.append(get_tf_sum(tf_dict, response))\n",
    "            \n",
    "    return tf_sum_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_originality_tf_only(df, stopwords_list, remove_sw, join_list):\n",
    "    originality_rating_df = get_cleaned_responses(df, stopwords_list, remove_sw, join_list)\n",
    "    responses = originality_rating_df['response_processed'].tolist()\n",
    "    responses_tokenized = [item for sublist in responses for item in sublist]\n",
    "\n",
    "    tf_dict = get_tf_dict(responses_tokenized)\n",
    "    \n",
    "    tf_sum_list = get_tf_sum_list(tf_dict, responses)\n",
    "\n",
    "    originality_rating_df['tf_sum'] = tf_sum_list\n",
    "        \n",
    "    return originality_rating_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_originality_tf_only(data_brick, stopwords_spacy, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Originality Algo 2\n",
    "### tf-idf scikit-learn\n",
    "### participant as document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_scikit_learn(cleaned_responses):\n",
    "    tfIdfVectorizer=TfidfVectorizer(use_idf=True)\n",
    "    tfIdf = tfIdfVectorizer.fit_transform(cleaned_responses)\n",
    "    df = pd.DataFrame(tfIdf.toarray(), columns=tfIdfVectorizer.get_feature_names())\n",
    "#     df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "#     df = df.sort_values('TF-IDF', ascending=False)\n",
    "    display(df)\n",
    "    \n",
    "#     feature_names = tfIdfVectorizer.get_feature_names()\n",
    "#     for col in tfIdf.nonzero()[1]:\n",
    "#         print (feature_names[col], ' - ', tfIdf[0, col])\n",
    "\n",
    "    print(tfIdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_originality_tfidf_scikit_learn(df, stopwords_list, remove_sw, join_list):\n",
    "    originality_rating_df = get_cleaned_responses(df, stopwords_list, remove_sw, join_list)\n",
    "    responses = []\n",
    "    \n",
    "    id_list = get_id_list(df)\n",
    "    \n",
    "    for participant in id_list:\n",
    "        temp_df = originality_rating_df.loc[originality_rating_df['id'] == participant]\n",
    "        temp_list = temp_df['response_processed'].tolist()\n",
    "        temp_list = ' '.join(temp_list)\n",
    "        responses.append(temp_list)\n",
    "        \n",
    "    print(responses)\n",
    "    \n",
    "    tfidf_scikit_learn(responses)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_originality_tfidf_scikit_learn(data_brick, stopwords_spacy, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Originality Algo 3\n",
    "### tf-idf scikit-learn + clustering\n",
    "### participant as document\n",
    "\n",
    "\n",
    "### Potential for Flexibility Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_scikit_learn_clustering(num_clusters, cleaned_responses):\n",
    "    tfidf_vectorizer=TfidfVectorizer(use_idf=True)\n",
    "    tfidf = tfidf_vectorizer.fit_transform(cleaned_responses)\n",
    "#     df = pd.DataFrame(tfIdf.toarray(), columns=tfIdfVectorizer.get_feature_names())\n",
    "#     df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "#     df = df.sort_values('TF-IDF', ascending=False)\n",
    "#     print (df)\n",
    "    \n",
    "    number_of_clusters = num_clusters\n",
    "    kmeans = KMeans(n_clusters = num_clusters).fit(tfidf)\n",
    "    \n",
    "    order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = tfidf_vectorizer.get_feature_names()\n",
    "    for i in range(number_of_clusters):\n",
    "        top_ten_words = [terms[ind] for ind in order_centroids[i, :5]]\n",
    "        print(\"Cluster {}: {}\".format(i, ' '.join(top_ten_words)))\n",
    "        \n",
    "    results = pd.DataFrame()\n",
    "    results['text'] = cleaned_responses\n",
    "    results['category'] = kmeans.labels_\n",
    "    \n",
    "    results_dict = {k: g[\"text\"].tolist() for k,g in results.groupby(\"category\")}\n",
    "    \n",
    "    test = pd.DataFrame(list(results_dict.items()),columns = ['category','responses']) \n",
    "    display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_originality_tfidf_scikit_learn_clustering(df, stopwords_list, num_clusters, remove_sw, join_list):\n",
    "    originality_rating_df = get_cleaned_responses(df, stopwords_list, remove_sw, join_list)\n",
    "    responses_split = originality_rating_df['response_processed'].tolist()\n",
    "    responses = []\n",
    "    \n",
    "    id_list = get_id_list(df)\n",
    "    \n",
    "    for participant in id_list:\n",
    "        temp_df = originality_rating_df.loc[originality_rating_df['id'] == participant]\n",
    "        temp_list = temp_df['response_processed'].tolist()\n",
    "        temp_list = ' '.join(temp_list)\n",
    "        responses.append(temp_list)\n",
    "        \n",
    "    display(originality_rating_df)\n",
    "    \n",
    "    tfidf_scikit_learn_clustering(num_clusters, responses_split)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_originality_tfidf_scikit_learn_clustering(data_brick, stopwords_spacy, 15, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Originality Algo 4\n",
    "### Counter Vectorizer + clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv_freqs_list(df):\n",
    "    z = 5\n",
    "    # Work in Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counts_vector(num_clusters, responses):\n",
    "    # initialize CountVectorizer object\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    # vectorize the phrases\n",
    "    word_count = count_vectorizer.fit_transform(responses)\n",
    "    \n",
    "    # elbow method to visualize and find out how many clusters to use\n",
    "    visualizer = KElbowVisualizer(KMeans(), k=(10,35), timings=False)\n",
    "    visualizer.fit(word_count.toarray())       \n",
    "#     visualizer.show()\n",
    "\n",
    "    # nltk kmeans cosine distance implementation\n",
    "    number_of_clusters = num_clusters\n",
    "    kmeans = KMeansClusterer(number_of_clusters, distance=nltk.cluster.util.cosine_distance, repeats=25, avoid_empty_clusters=True)\n",
    "    assigned_clusters = kmeans.cluster(word_count.toarray(), assign_clusters=True)\n",
    "\n",
    "    # scikit-learn euclidean distance implementation\n",
    "#     kmeans = KMeans(n_clusters = num_clusters).fit(word_count)\n",
    "        \n",
    "    # cluster results scikit-learn\n",
    "    results = pd.DataFrame()\n",
    "    results['text'] = responses\n",
    "#     results['category'] = kmeans.labels_\n",
    "    results['category'] = assigned_clusters\n",
    "    \n",
    "    # create dictionary to organize the clusters with their respective phrases\n",
    "    results_dict = {k: g[\"text\"].tolist() for k,g in results.groupby(\"category\")}\n",
    "    \n",
    "    # df of the clusters and the \n",
    "    clusters_df = pd.DataFrame(list(results_dict.items()),columns = ['category','responses']) \n",
    "    \n",
    "    return clusters_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clustered_originality_score(originality_rating_df, column):\n",
    "    # get cluster df\n",
    "    clusters_df = get_counts_vector(22, originality_rating_df['response_processed'].tolist())\n",
    "\n",
    "    # create dictionary out of cluster df\n",
    "    clusters = dict(zip(clusters_df.category, clusters_df.responses))\n",
    "        \n",
    "    # initialize empty dictionary to store the score for a category\n",
    "    clusters_scores = dict.fromkeys(clusters)\n",
    "    \n",
    "    # get the average cosine distance for a cluster\n",
    "    for key in clusters:\n",
    "        score = 1.0\n",
    "        score = score - (len(clusters[key])/len(originality_rating_df.index))\n",
    "        clusters_scores[key] = score\n",
    "        \n",
    "    # create dictionary to store a phrase and its new novelty score \n",
    "    # new score is the average of the responses in one cluster\n",
    "    phrase_scores_dict = {}\n",
    "    for key in clusters:\n",
    "        for phrase in clusters[key]:\n",
    "            phrase_scores_dict[phrase] = clusters_scores[key]\n",
    "            \n",
    "    # make a list that matches the one in the current dataframe\n",
    "    # return list to be added to dataframe\n",
    "    df_phrases_scores_list = [] \n",
    "    for phrase in originality_rating_df['response_processed'].tolist():\n",
    "        df_phrases_scores_list.append(phrase_scores_dict[phrase])\n",
    "    \n",
    "    display(clusters_df)\n",
    "            \n",
    "    return list(df_phrases_scores_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_originality_count_vectorizer(df, stopwords_list, remove_sw, join_list):\n",
    "    originality_rating_df = get_cleaned_responses(df, stopwords_list, remove_sw, join_list)\n",
    "    responses = originality_rating_df['response_processed'].tolist()\n",
    "    originality_rating_df = originality_rating_df[originality_rating_df.astype(str)['response_processed'] != '']\n",
    "    \n",
    "#     responses_tokenized = [item for sublist in responses for item in sublist]\n",
    "    \n",
    "    originality_rating_df['counts_vectorizer_freq'] = get_clustered_originality_score(originality_rating_df, responses)\n",
    "        \n",
    "    return originality_rating_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_originality_count_vectorizer(data_brick, stopwords_spacy, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Results into Excel Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_list = ['cup', 'key', 'rope', 'brick', 'chair', 'pencil', 'shoe']\n",
    "data_list = [data_cup, data_key, data_rope, data_brick, data_chair, data_pencil, data_shoe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write results df of each dataset for specific methods\n",
    "def write_results_excel(method, stopwords, remove_sw, join_list):\n",
    "    # change this when writing new sheet\n",
    "    writer = pd.ExcelWriter('originality_cv_freq_cluster_results.xlsx', engine='xlsxwriter')\n",
    "\n",
    "    for i in range(len(prompts_list)):\n",
    "        df = method(data_list[i], stopwords, remove_sw, join_list)\n",
    "        df.to_excel(writer, sheet_name = prompts_list[i], index = False)\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_results_excel(get_originality_count_vectorizer, stopwords_spacy, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algo Design Brainstorming:\n",
    "* Algo Design w/o scikit-learn\n",
    "    * asdlfjakl;fjka\n",
    "    * asdfasf\n",
    "\n",
    "* Algo Design w/ scikit-learn\n",
    "\n",
    "To Do List\n",
    "- [x] write basic algorithm, sum of term frequency\n",
    "    - frequency based on all phrases\n",
    "- [x] write tfidf clustering method \n",
    "    - repurpose for flexibility\n",
    "- [x] write counter vectorizer clustering method\n",
    "- [x] write method to score responses by frequency\n",
    "    - number of responses in a cluster/number of total responses subtracted from 1\n",
    "    - subtract from 1 to get the reverse mapping\n",
    "- [ ] experiment with a greater cluster size\n",
    "- [ ] update stop words list to include \"use\" and \"thing\"\n",
    "- [ ] figure out how many times to run kmeans\n",
    "    - cross validation\n",
    "    - then averaging the results of all the iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4fb9038e648b520597eaf423817a5ee4bf2adb29cdca822101c12482799b056"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
