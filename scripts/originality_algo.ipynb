{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Originality Algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm to Automate Originality Scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "from functools import reduce\n",
    "import openpyxl\n",
    "import xlsxwriter\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import ngrams\n",
    "from nltk import ngrams, FreqDist\n",
    "from nltk.lm import NgramCounter\n",
    "import string\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "from collections import Counter\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put Data from Excel Sheet into Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# individual df's for each sheet\n",
    "\n",
    "# when on pc\n",
    "data_cup = pd.read_excel(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/autdata_test.xlsx\", sheet_name = \"Cup\")\n",
    "data_key = pd.read_excel(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/autdata_test.xlsx\", sheet_name = \"Key\")\n",
    "data_rope = pd.read_excel(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/autdata_test.xlsx\", sheet_name = \"Rope\")\n",
    "data_brick = pd.read_excel(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/autdata_test.xlsx\", sheet_name = \"Brick\")\n",
    "data_chair = pd.read_excel(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/autdata_test.xlsx\", sheet_name = \"Chair\")\n",
    "data_pencil = pd.read_excel(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/autdata_test.xlsx\", sheet_name = \"Pencil\")\n",
    "data_shoe = pd.read_excel(\"C:/Users/jhec8/Documents/Northwestern_SROP/AUT-Scoring/data/autdata_test.xlsx\", sheet_name = \"Shoe\")\n",
    "\n",
    "# when on mac\n",
    "# data_cup = pd.read_excel(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/autdata_test.xlsx\", sheet_name = \"Cup\", engine='openpyxl')\n",
    "# data_key = pd.read_excel(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/autdata_test.xlsx\", sheet_name = \"Key\", engine='openpyxl')\n",
    "# data_rope = pd.read_excel(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/autdata_test.xlsx\", sheet_name = \"Rope\", engine='openpyxl')\n",
    "# data_brick = pd.read_excel(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/autdata_test.xlsx\", sheet_name = \"Brick\", engine='openpyxl')\n",
    "# data_chair = pd.read_excel(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/autdata_test.xlsx\", sheet_name = \"Chair\", engine='openpyxl')\n",
    "# data_pencil = pd.read_excel(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/autdata_test.xlsx\", sheet_name = \"Pencil\", engine='openpyxl')\n",
    "# data_shoe = pd.read_excel(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/AUT-Scoring/data/autdata_test.xlsx\", sheet_name = \"Shoe\", engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk corpus stop words\n",
    "stopwords_nltk = stopwords.words('english')\n",
    "# spacy stop words\n",
    "stopwords_spacy = STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to clean the responses\n",
    "def process_text(text, stopwords_list, remove_sw, join_list, stem = True):\n",
    "    # tokenize text, lemmanize words, removing punctuation, remove stop words, lowercase all words\n",
    "\n",
    "    # hardcorded for special situations\n",
    "    text = re.sub(\"wedging\",\"wedge\", text)\n",
    "    text = re.sub(\"exersizing\",\"exercising\", text)\n",
    "    \n",
    "    text = re.sub(\"/|-\",\" \", text)\n",
    "    text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    if remove_sw:\n",
    "        tokens = [word for word in tokens if word not in stopwords_list]\n",
    "\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "\n",
    "    if stem:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "#         stemmer = PorterStemmer()\n",
    "#         tokens = [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "    if join_list:\n",
    "        tokens = ' '.join(tokens)\n",
    " \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to get a list of participants\n",
    "def get_id_list(df):\n",
    "    id_list = df['id'].unique()\n",
    "    id_list = sorted(id_list)\n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to add a new column\n",
    "# new column are cleaned responses\n",
    "def get_cleaned_responses(df, stopwords_list, remove_sw, join_list):\n",
    "    # id_df = df[df.id == id]\n",
    "    df_processed = df.copy(deep=True)\n",
    "    responses = df['response'].tolist()\n",
    "\n",
    "    # make list of processed responses\n",
    "    for response in range(len(responses)):\n",
    "        responses[response] = process_text(responses[response], stopwords_list, remove_sw, join_list, True)\n",
    "\n",
    "    # add list as column in df\n",
    "    df_processed['response_processed'] = responses\n",
    "\n",
    "    return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Models for Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model\n",
    "word_model_twitter25 = api.load(\"glove-twitter-25\")\n",
    "\n",
    "# on pc\n",
    "# word_model_google = KeyedVectors.load_word2vec_format(\"C:/Users/jhec8/Documents/Northwestern_SROP/GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "\n",
    "# on mac\n",
    "# word_model = KeyedVectors.load_word2vec_format(\"/Users/johnhenrycruz/Desktop/Northwestern_SROP/GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary of counts for each word in model\n",
    "twitter25_dict = {}\n",
    "for i in range(len(word_model_twitter25)):\n",
    "    twitter25_dict[word_model_twitter25.index_to_key[i]] = word_model_twitter25.key_to_index[word_model_twitter25.index_to_key[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the frequency of each word in dictionary\n",
    "total_words = 0\n",
    "for key in twitter25_dict:\n",
    "    total_words = total_words + twitter25_dict[key]\n",
    "    \n",
    "for key in twitter25_dict:\n",
    "    twitter25_dict[key] = twitter25_dict[key]/total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Originality Algo 1\n",
    "### tf-idf scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_scikit_learn(cleaned_responses):\n",
    "    tfIdfVectorizer=TfidfVectorizer(use_idf=True)\n",
    "    tfIdf = tfIdfVectorizer.fit_transform(cleaned_responses)\n",
    "#     df = pd.DataFrame(tfIdf.toarray(), columns=tfIdfVectorizer.get_feature_names())\n",
    "#     df = pd.DataFrame(tfIdf[0].T.todense(), index=tfIdfVectorizer.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "#     df = df.sort_values('TF-IDF', ascending=False)\n",
    "#     print (df)\n",
    "    \n",
    "    number_of_clusters=2\n",
    "    kmeans = KMeans(n_clusters=2).fit(tfIdf)\n",
    "    \n",
    "    order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "    terms = tfIdfVectorizer.get_feature_names()\n",
    "    for i in range(number_of_clusters):\n",
    "        top_ten_words = [terms[ind] for ind in order_centroids[i, :5]]\n",
    "        print(\"Cluster {}: {}\".format(i, ' '.join(top_ten_words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_originality_tfidf_scikit_learn(df, stopwords_list, remove_sw, join_list):\n",
    "    novel_rating_df = get_cleaned_responses(df, stopwords_list, remove_sw, join_list)\n",
    "    responses = novel_rating_df['response_processed'].tolist()\n",
    "#     responses = [item for sublist in responses for item in sublist]\n",
    "    \n",
    "    print(responses)\n",
    "    \n",
    "    tfidf_scikit_learn(responses)\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['turn lock', 'cut box', 'pendant', 'open door', 'cut tape box', 'stab', 'scratch car', 'door opener', 'knife', 'letter opener', 'chime', 'gift', 'unlock door', 'open soup', 'start car', 'wear necklace', 'open door', 'use decoration', 'paperweight', 'scratch lottery ticket', 'open', 'unlock door', 'scratch scratch ticket', 'cut open plastic', 'scratch someone car dont like', 'unlock door', 'scratch lottery ticket', 'key car', 'decoration', 'necklace', 'lock', 'necklace', 'bookmark', 'decoration', 'weapon', 'unlock', 'cut', 'throw', 'lock unlock door', 'pendant', 'weapon', 'cutting utensil', 'unlock door', 'open box', 'dog attention', 'poke', 'unlock', 'use art piece', 'jingle amuse baby', 'unlock door', 'scratch', 'fidget', 'distract cat', 'unlock lock', 'cut', 'comb', 'break', 'open door', 'stab', 'pry open tab tough', 'carve art surface', 'slash someone tire', 'opening door', 'weapon', 'bottle opener', 'currency', 'opening chest', 'melt scrap metal', 'unlock door', 'start car', 'cut small', 'belong', 'play pretend', 'entertain baby noise', '', 'unlock thing', 'open shotgun beer', 'open package', 'fight finger', 'jingle distract baby', 'key car', 'scratch stuff', 'unlock door', 'key car', 'distract baby', 'noise']\n",
      "Cluster 0: scratch cut car open weapon\n",
      "Cluster 1: door unlock open lock opening\n"
     ]
    }
   ],
   "source": [
    "get_originality_tfidf_scikit_learn(data_key, stopwords_spacy, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Results into Excel Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_list = ['cup', 'key', 'rope', 'brick', 'chair', 'pencil', 'shoe']\n",
    "data_list = [data_cup, data_key, data_rope, data_brick, data_chair, data_pencil, data_shoe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write results df of each dataset for specific methods\n",
    "def write_results_excel(method, stopwords, model):\n",
    "    # change this when writing new sheet\n",
    "    writer = pd.ExcelWriter('novelty_algo_2_results.xlsx', engine='xlsxwriter')\n",
    "\n",
    "    for i in range(len(prompts_list)):\n",
    "        df = method(data_list[i], prompts_list[i], stopwords, model)\n",
    "        df.to_excel(writer, sheet_name = prompts_list[i], index = False)\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_results_excel(get_novelty_word2vec_avg, stopwords_spacy, word_model_twitter25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_results_excel(get_novelty_word2vec_sif_cosinesim, stopwords_spacy, word_model_twitter25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algo Design Brainstorming:\n",
    "* Algo Design w/o scikit-learn\n",
    "    * asdlfjakl;fjka\n",
    "    * asdfasf\n",
    "\n",
    "* Algo Design w/ scikit-learn\n",
    "\n",
    "* To Do List\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f4fb9038e648b520597eaf423817a5ee4bf2adb29cdca822101c12482799b056"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
